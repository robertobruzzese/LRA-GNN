import argparse
import os
import torch
from torch.utils.data import DataLoader
from models.progressive_rl import ProgressiveRLAgent
from training.train_rl import train_prlae
from dataset.embedding_dataset import EmbeddingDataset
from models.classifier import AgeGroupClassifier
from datetime import datetime
import glob
import re

# ğŸ”§ Argomento per scegliere il dataset
parser = argparse.ArgumentParser()
parser.add_argument("--dataset", type=str, default="MORPH", help="Nome del dataset (MORPH o FGNET)")
args = parser.parse_args()
dataset_name = args.dataset.upper()

# âš™ï¸ Dispositivo
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

# ğŸ“‚ Seleziona cartella embeddings corretta
embedding_dir = "embeddings/train" if dataset_name == "MORPH" else "embeddings_FGNET/train"

# ğŸ“¥ Carica dataset di embedding
embedding_dataset = EmbeddingDataset(embedding_dir)
embedding_loader = DataLoader(embedding_dataset, batch_size=1, shuffle=True)

# ğŸ”¢ Calcola dinamicamente la dimensione dello stato
first_embedding, _ = embedding_dataset[0]
embedding_dim = first_embedding.shape[0]
state_dim = embedding_dim + 6  # embedding + delta_x + delta_y + pos_r + pos_c
action_dim = 5  # su, giÃ¹, sinistra, destra, resta

# ğŸ” Classificatore pre-addestrato
classifier = AgeGroupClassifier(input_dim=128).to(device)
classifier_path = os.path.join("checkpoints", dataset_name, "classifier.pth")


if os.path.exists(classifier_path):
    try:
        classifier.load_state_dict(torch.load(classifier_path, map_location=device))
        classifier.eval()
        print(f"âœ… Classificatore caricato da {classifier_path}")
    except RuntimeError as e:
        print("âš ï¸ Errore nel caricamento del classificatore:", str(e))
        print("ğŸ‘‰ Esegui di nuovo `train_classifier.py` per rigenerare il file.")
        exit(1)
else:
    print(f"âš ï¸ Nessun classificatore trovato in {classifier_path}: esegui prima train_classifier.py")
    exit(1)


# ğŸ¤– Istanzia lâ€™agente RL
agent = ProgressiveRLAgent(
    state_dim=state_dim,
    action_dim=action_dim,
    device=device,
    classifier=classifier
)
agent.q_network.to(device)
# ğŸ” Carica automaticamente il checkpoint piÃ¹ recente se esiste (ordinando per episodio)
checkpoint_dir = os.path.join("checkpoints", dataset_name.upper())
checkpoints = glob.glob(os.path.join(checkpoint_dir, "rl_agent_partial_*.pth"))

# ğŸ”¢ Funzione per estrarre il numero di episodio dal nome file
def extract_episode_num(path):
    match = re.search(r'rl_agent_partial_(\d+)_', path)
    return int(match.group(1)) if match else -1

# ğŸ“‹ Ordina i checkpoint in base all'episodio (non alfabeticamente)
checkpoints = sorted(checkpoints, key=extract_episode_num)

if checkpoints:
    last_checkpoint = checkpoints[-1]
    agent.load(last_checkpoint)
    print(f"ğŸ“¥ Checkpoint caricato da {last_checkpoint}")
    start_step = extract_episode_num(last_checkpoint)
else:
    print("ğŸš€ Nessun checkpoint trovato. Inizio training da zero.")
    start_step = 0

# ğŸš€ Allena lâ€™agente
if __name__ == "__main__":
    # Esegui 4 batch da 50 episodi
    best_accuracy = 0.0
    for step in range(start_step, 200, 50):
        best_accuracy = train_prlae(
            agent=agent,
            dataloader=embedding_loader,
            device=device,
            dataset_name=dataset_name,
            num_episodes=50,
            start_episode=step,
            save_every=50,
            best_accuracy=best_accuracy
        )


    #train_prlae(
    #    agent=agent,
    #    dataloader=embedding_loader,
    #    device=device,
    #    dataset_name=dataset_name,
    #    num_episodes=200
    #)
     # ğŸ’¾ Salva il modello in cartella diversa per dataset
    # ğŸ”¹ Crea la directory di salvataggio in base al dataset
    checkpoint_dir = os.path.join("checkpoints", dataset_name.upper())  # garantisce maiuscolo
    os.makedirs(checkpoint_dir, exist_ok=True)
    # ğŸ”¹ Costruisci il percorso del file di salvataggio
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    save_path = os.path.join(checkpoint_dir, f"rl_agent_{timestamp}.pth")

    # ğŸ”¹ Salva l'agente
    agent.save(save_path)
    print(f"\nğŸ’¾ RL agent salvato in: {save_path}")
    


# ğŸ’¾ (Opzionale) Salva il modello addestrato
# agent.save("checkpoints/rl_agent_best.pth")

print("\nğŸ Training RL completato!")
